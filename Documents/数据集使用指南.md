# 数据集使用指南

## 快速开始

### 1. 统一数据集接口使用

```python
from datasets.universal_dataset import create_dataset

# 创建VOC2012数据集
voc_dataset = create_dataset(
    dataset_type='voc2012',
    root_dir='./datasets/VOC2012',
    split='train',
    image_size=(256, 256)
)

# 创建ADE20K数据集
ade_dataset = create_dataset(
    dataset_type='ade20k',
    root_dir='./datasets/ADE20K',
    split='training',
    image_size=(512, 512)
)

# 创建Cityscapes数据集
cityscapes_dataset = create_dataset(
    dataset_type='cityscapes',
    root_dir='./datasets/Cityscapes',
    split='train',
    image_size=(1024, 512)
)
```

### 2. 配置文件使用

```python
from configs.config_manager import ConfigManager

# 加载多数据集配置
config = ConfigManager('configs/multi_dataset_example.yaml')

# 根据配置创建数据集
dataset = create_dataset(
    dataset_type=config.dataset.type,
    root_dir=config.dataset.root_dir,
    split=config.dataset.split,
    image_size=config.dataset.image_size
)
```

## 数据集格式要求

### VOC2012格式
```
VOC2012/
├── JPEGImages/
│   ├── 2007_000032.jpg
│   └── ...
├── SegmentationClass/
│   ├── 2007_000032.png
│   └── ...
└── ImageSets/
    └── Segmentation/
        ├── train.txt
        ├── val.txt
        └── trainval.txt
```

### ADE20K格式
```
ADE20K/
├── images/
│   ├── training/
│   │   ├── ADE_train_00000001.jpg
│   │   └── ...
│   └── validation/
│       └── ...
├── annotations/
│   ├── training/
│   │   ├── ADE_train_00000001.png
│   │   └── ...
│   └── validation/
│       └── ...
```

### Cityscapes格式
```
cityscapes/
├── leftImg8bit/
│   ├── train/
│   │   ├── aachen/
│   │   │   ├── aachen_000000_000019_leftImg8bit.png
│   │   │   └── ...
│   │   └── ...
│   └── ...
├── gtFine/
│   ├── train/
│   │   ├── aachen/
│   │   │   ├── aachen_000000_000019_gtFine_labelTrainIds.png
│   │   │   └── ...
│   │   └── ...
│   └── ...
```

## 实验建议

### 阶段1：快速验证（VOC2012）
- **目标**：验证模型架构和训练流程
- **配置**：低分辨率(256x256)，小批次(8)，短训练(10 epochs)
- **时间**：1-2小时
- **指标**：mIoU > 60%表示基本正常

### 阶段2：全面评估（ADE20K）
- **目标**：验证模型泛化能力和多类别性能
- **配置**：中分辨率(512x512)，适中批次(16)，充分训练(50 epochs)
- **时间**：4-8小时
- **指标**：mIoU > 30%表示性能良好

### 阶段3：精度测试（Cityscapes）
- **目标**：追求最高精度和细节优化
- **配置**：高分辨率(1024x512)，小批次(8)，长训练(100 epochs)
- **时间**：8-16小时
- **指标**：mIoU > 70%表示达到良好水平

## 性能对比参考

### 不同数据集的预期性能

| 数据集     | 轻量级模型 | 标准模型 | 重型模型 |
| ---------- | ---------- | -------- | -------- |
| VOC2012    | 60-70%     | 70-80%   | 80-85%   |
| ADE20K     | 25-35%     | 35-45%   | 45-50%   |
| Cityscapes | 65-75%     | 75-85%   | 85-90%   |

### 训练时间预期（单GPU）

| 数据集     | 图像尺寸 | 批次大小 | 每epoch时间 | 总训练时间 |
| ---------- | -------- | -------- | ----------- | ---------- |
| VOC2012    | 256x256  | 8        | 2-5分钟     | 20-50分钟  |
| ADE20K     | 512x512  | 16       | 15-30分钟   | 12-25小时  |
| Cityscapes | 1024x512 | 8        | 30-60分钟   | 50-100小时 |

## 常见问题解决

### 1. 数据集路径问题
```python
# 检查数据集是否存在
import os

def check_dataset(dataset_type, root_dir):
    if dataset_type == 'voc2012':
        required_dirs = ['JPEGImages', 'SegmentationClass', 'ImageSets/Segmentation']
    elif dataset_type == 'ade20k':
        required_dirs = ['images', 'annotations']
    elif dataset_type == 'cityscapes':
        required_dirs = ['leftImg8bit', 'gtFine']

    for dir_name in required_dirs:
        dir_path = os.path.join(root_dir, dir_name)
        if not os.path.exists(dir_path):
            print(f"Missing directory: {dir_path}")
            return False
    return True
```

### 2. 内存不足问题
```python
# 减少批次大小和图像尺寸
config = {
    'batch_size': 4,  # 从16减少到4
    'image_size': (256, 256),  # 从(512, 512)减少到(256, 256)
    'num_workers': 2,  # 减少数据加载进程数
}
```

### 3. 类别数不匹配问题
```python
# 确保模型类别数与数据集一致
dataset_classes = {
    'voc2012': 21,
    'ade20k': 150,
    'cityscapes': 19
}

model_config = {
    'num_classes': dataset_classes[dataset_type]
}
```

## 最佳实践建议

### 1. 渐进式分辨率训练
```python
# 阶段1：低分辨率快速收敛
stage1_config = {'image_size': (256, 256), 'epochs': 20}

# 阶段2：中分辨率精调
stage2_config = {'image_size': (512, 512), 'epochs': 30}

# 阶段3：高分辨率最终优化
stage3_config = {'image_size': (1024, 512), 'epochs': 20}
```

### 2. 多数据集联合训练
```python
# 先在大数据集上预训练
pretrain_dataset = 'ade20k'  # 25K样本

# 再在目标数据集上微调
finetune_dataset = 'cityscapes'  # 5K样本，高质量
```

### 3. 数据增强策略
```python
data_augmentation = {
    'voc2012': 'moderate',    # 中等增强，避免过拟合
    'ade20k': 'standard',     # 标准增强，保持多样性
    'cityscapes': 'strong'    # 强增强，充分利用高质量数据
}
```

## 下一步计划

1. **扩展数据集支持**：添加COCO-Stuff、PASCAL Context等
2. **优化数据加载**：实现多进程并行加载和缓存机制
3. **增强预处理**：添加自动数据增强和在线难样本挖掘
4. **评估工具**：实现自动化的多数据集评估和对比
5. **可视化工具**：添加预测结果可视化和错误分析工具

## 参考资源

- [VOC2012数据集官网](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)
- [ADE20K数据集官网](http://groups.csail.mit.edu/vision/datasets/ADE20K/)
- [Cityscapes数据集官网](https://www.cityscapes-dataset.com/)
- [语义分割评估指标说明](https://paperswithcode.com/task/semantic-segmentation)
