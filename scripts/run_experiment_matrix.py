"""
å®éªŒçŸ©é˜µè‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
æ”¯æŒæ‰¹é‡è¿è¡Œå¤šä¸ªå®éªŒé…ç½®ï¼Œè‡ªåŠ¨è®°å½•ç»“æœå’Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""
import os
import sys
import yaml
import json
import time
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


class ExperimentRunner:
    """å®éªŒè‡ªåŠ¨åŒ–è¿è¡Œå™¨"""

    def __init__(self,
                 base_dir: str = ".",
                 results_dir: str = "results/experiment_matrix",
                 gpu_ids: List[int] = [0]):
        self.base_dir = Path(base_dir)
        self.results_dir = Path(results_dir)
        self.gpu_ids = gpu_ids
        self.current_gpu = 0

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # å®éªŒè®°å½•
        self.experiment_log = []

    def get_next_gpu(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨GPU"""
        gpu_id = self.gpu_ids[self.current_gpu % len(self.gpu_ids)]
        self.current_gpu += 1
        return gpu_id

    def load_experiment_configs(self, config_pattern: str) -> List[Dict]:
        """åŠ è½½å®éªŒé…ç½®æ–‡ä»¶"""
        configs = []
        config_files = list(self.base_dir.glob(config_pattern))

        for config_file in config_files:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    config['config_file'] = str(config_file)
                    config['experiment_id'] = config_file.stem
                    configs.append(config)
            except Exception as e:
                print(f"Warning: Failed to load {config_file}: {e}")

        return configs

    def run_single_experiment(self, config: Dict, timeout: int = 7200) -> Dict:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        experiment_id = config['experiment_id']
        config_file = config['config_file']
        gpu_id = self.get_next_gpu()

        print(f"\\n{'='*50}")
        print(f"Running Experiment: {experiment_id}")
        print(f"Config: {config_file}")
        print(f"GPU: {gpu_id}")
        print(f"{'='*50}")

        # å‡†å¤‡å®éªŒç›®å½•
        exp_dir = self.results_dir / experiment_id
        exp_dir.mkdir(exist_ok=True)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ„å»ºè¿è¡Œå‘½ä»¤
        cmd = [
            "python", "train_complete.py",
            "--config", config_file,
            "--gpu_id", str(gpu_id),
            "--output_dir", str(exp_dir)
        ]

        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)

        try:
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                cmd,
                cwd=self.base_dir,
                env=env,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            duration = end_time - start_time

            # è§£æç»“æœ
            success = result.returncode == 0
            stdout = result.stdout
            stderr = result.stderr

            # å°è¯•ä»è¾“å‡ºä¸­æå–æ€§èƒ½æŒ‡æ ‡
            metrics = self.extract_metrics_from_output(stdout)

            # è®°å½•å®éªŒç»“æœ
            experiment_result = {
                'experiment_id': experiment_id,
                'config_file': config_file,
                'gpu_id': gpu_id,
                'start_time': datetime.fromtimestamp(start_time).isoformat(),
                'end_time': datetime.fromtimestamp(end_time).isoformat(),
                'duration': duration,
                'success': success,
                'metrics': metrics,
                'stdout': stdout[-2000:] if stdout else "",  # ä¿ç•™æœ€å2000å­—ç¬¦
                'stderr': stderr[-1000:] if stderr else "",   # ä¿ç•™æœ€å1000å­—ç¬¦
                'return_code': result.returncode
            }

            # ä¿å­˜å®éªŒç»“æœ
            with open(exp_dir / "experiment_result.json", 'w') as f:
                json.dump(experiment_result, f, indent=2)

            print(f"âœ… Experiment {experiment_id} completed successfully!")
            if metrics:
                print(f"   Best mIoU: {metrics.get('best_miou', 'N/A'):.4f}")
                print(f"   Final Loss: {metrics.get('final_loss', 'N/A'):.4f}")

            return experiment_result

        except subprocess.TimeoutExpired:
            print(f"âŒ Experiment {experiment_id} timed out after {timeout}s")
            return {
                'experiment_id': experiment_id,
                'config_file': config_file,
                'success': False,
                'error': 'timeout',
                'duration': timeout
            }

        except Exception as e:
            print(f"âŒ Experiment {experiment_id} failed: {e}")
            return {
                'experiment_id': experiment_id,
                'config_file': config_file,
                'success': False,
                'error': str(e),
                'duration': time.time() - start_time
            }

    def extract_metrics_from_output(self, stdout: str) -> Dict:
        """ä»è¾“å‡ºä¸­æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}

        if not stdout:
            return metrics

        lines = stdout.split('\\n')
        for line in lines:
            # æå–æœ€ä½³mIoU
            if 'Best mIoU:' in line:
                try:
                    metrics['best_miou'] = float(line.split('Best mIoU:')[1].split()[0])
                except:
                    pass

            # æå–æœ€ç»ˆloss
            if 'Final Loss:' in line:
                try:
                    metrics['final_loss'] = float(line.split('Final Loss:')[1].split()[0])
                except:
                    pass

            # æå–è®­ç»ƒæ—¶é—´
            if 'Training completed in' in line:
                try:
                    time_str = line.split('Training completed in')[1].split()[0]
                    metrics['training_time'] = float(time_str)
                except:
                    pass

        return metrics

    def run_experiment_matrix(self,
                              config_pattern: str = "configs/*.yaml",
                              max_parallel: int = 1,
                              timeout: int = 7200) -> List[Dict]:
        """è¿è¡Œå®éªŒçŸ©é˜µ"""
        # åŠ è½½æ‰€æœ‰é…ç½®
        configs = self.load_experiment_configs(config_pattern)
        print(f"Found {len(configs)} experiment configurations")

        if not configs:
            print("No valid configurations found!")
            return []

        # è¿è¡Œå®éªŒ
        results = []
        for i, config in enumerate(configs):
            print(f"\\nProgress: {i+1}/{len(configs)}")
            result = self.run_single_experiment(config, timeout)
            results.append(result)
            self.experiment_log.append(result)

            # ä¿å­˜ä¸­é—´ç»“æœ
            self.save_experiment_summary(results)

        print(f"\\nğŸ‰ All experiments completed!")
        print(f"Results saved to: {self.results_dir}")

        return results

    def save_experiment_summary(self, results: List[Dict]):
        """ä¿å­˜å®éªŒæ€»ç»“"""
        summary_file = self.results_dir / "experiment_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(results, f, indent=2)

        # ç”ŸæˆCSVæŠ¥å‘Š
        self.generate_csv_report(results)

        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self.generate_visualization_report(results)

    def generate_csv_report(self, results: List[Dict]):
        """ç”ŸæˆCSVæ ¼å¼çš„å®éªŒæŠ¥å‘Š"""
        rows = []
        for result in results:
            if result['success']:
                metrics = result.get('metrics', {})
                row = {
                    'experiment_id': result['experiment_id'],
                    'best_miou': metrics.get('best_miou', 0.0),
                    'final_loss': metrics.get('final_loss', 0.0),
                    'duration_hours': result.get('duration', 0) / 3600,
                    'success': result['success']
                }
                rows.append(row)

        if rows:
            df = pd.DataFrame(rows)
            csv_file = self.results_dir / "experiment_results.csv"
            df.to_csv(csv_file, index=False)

            # æ’åºå¹¶æ˜¾ç¤ºtopç»“æœ
            df_sorted = df.sort_values('best_miou', ascending=False)
            print(f"\\nğŸ“Š Top 5 Results:")
            print(df_sorted.head().to_string(index=False))

    def generate_visualization_report(self, results: List[Dict]):
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        successful_results = [r for r in results if r['success']]
        if len(successful_results) < 2:
            return

        # å‡†å¤‡æ•°æ®
        exp_ids = [r['experiment_id'] for r in successful_results]
        mious = [r.get('metrics', {}).get('best_miou', 0) for r in successful_results]
        durations = [r.get('duration', 0) / 3600 for r in successful_results]  # è½¬æ¢ä¸ºå°æ—¶

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # mIoUå¯¹æ¯”
        ax1.bar(range(len(exp_ids)), mious)
        ax1.set_xlabel('Experiments')
        ax1.set_ylabel('Best mIoU')
        ax1.set_title('Experiment Performance Comparison')
        ax1.set_xticks(range(len(exp_ids)))
        ax1.set_xticklabels(exp_ids, rotation=45, ha='right')

        # è®­ç»ƒæ—¶é—´å¯¹æ¯”
        ax2.bar(range(len(exp_ids)), durations)
        ax2.set_xlabel('Experiments')
        ax2.set_ylabel('Duration (hours)')
        ax2.set_title('Training Time Comparison')
        ax2.set_xticks(range(len(exp_ids)))
        ax2.set_xticklabels(exp_ids, rotation=45, ha='right')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_file = self.results_dir / "experiment_comparison.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ğŸ“ˆ Visualization saved to: {plot_file}")


def main():
    parser = argparse.ArgumentParser(description='Run experiment matrix')
    parser.add_argument('--config_pattern', default='configs/*.yaml',
                        help='Pattern to match config files')
    parser.add_argument('--results_dir', default='results/experiment_matrix',
                        help='Directory to save results')
    parser.add_argument('--gpu_ids', nargs='+', type=int, default=[0],
                        help='GPU IDs to use')
    parser.add_argument('--timeout', type=int, default=7200,
                        help='Timeout per experiment in seconds')
    parser.add_argument('--max_parallel', type=int, default=1,
                        help='Maximum parallel experiments')

    args = parser.parse_args()

    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(
        results_dir=args.results_dir,
        gpu_ids=args.gpu_ids
    )

    # è¿è¡Œå®éªŒçŸ©é˜µ
    results = runner.run_experiment_matrix(
        config_pattern=args.config_pattern,
        max_parallel=args.max_parallel,
        timeout=args.timeout
    )

    # è¾“å‡ºæ€»ç»“
    successful = sum(1 for r in results if r['success'])
    total = len(results)

    print(f"\\nğŸ“‹ Experiment Summary:")
    print(f"   Total experiments: {total}")
    print(f"   Successful: {successful}")
    print(f"   Failed: {total - successful}")
    print(f"   Success rate: {successful/total*100:.1f}%")


if __name__ == "__main__":
    main()
